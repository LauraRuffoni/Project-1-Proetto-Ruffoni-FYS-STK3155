{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37f2d9ae-0224-4004-8315-677f85aaf532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f58c88a2570>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso, SGDRegressor\n",
    "from numpy import asarray\n",
    "\n",
    "# Make data\n",
    "np.random.seed(67)\n",
    "n = 100\n",
    "X = 2 * np.random.rand(n, 1)\n",
    "\n",
    "y = 2 + 3 * X \n",
    "\n",
    "# Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "# Splitting into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=32)\n",
    "\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121ab2f-d33f-495b-90ae-fbf793549d31",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ce969-49e7-444c-aac2-084f3110c92d",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa67947-af55-498f-9a70-b4284d0a84e6",
   "metadata": {},
   "source": [
    "### plain GD fixed learning rate to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0578e139-e29a-40cd-ba8d-dbe4f565af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00039722],\n",
       "       [2.99939781]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by hand\n",
    "def plain_GD(X_train, X_val, y_train, y_val, degree, lmbda):\n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train) + 2 * lmbda * beta\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            # Update beta\n",
    "            beta -= eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_GD(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "best_model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ceac15-7f5d-4e7f-900b-7d64fe90ab4e",
   "metadata": {},
   "source": [
    "### with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220f6e32-4677-44ca-8322-e415fc855b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having 0.0001 used eta = 0.01 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00008427],\n",
       "       [2.99971104]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plain_GD_with_momentum_OLS(X_train, X_val, y_train, y_val, degree, lmbda, momentum=0.9):\n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        change = np.zeros_like(beta)  # Initialize change for momentum\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train) + 2 * lmbda * beta\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            # Update beta using momentum\n",
    "            new_change = eta * gradient + momentum * change\n",
    "            beta = beta - new_change\n",
    "            change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_GD_with_momentum_OLS(X_train, X_val, y_train, y_val, 1, lmbda, momentum=0.9)\n",
    "best_model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcb7f1-5256-4da0-b106-1c29a4b24946",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c5e119a-cc48-4d14-b150-24aa8cdadac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.00038718]\n",
      " [2.99940154]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad\n",
    "def plain_Adagrad(X_train, X_val, y_train, y_val, degree, lmbda):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train) + 2 * lmbda * beta\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            # Accumulate squared gradients\n",
    "            accumulated_gradients += gradient ** 2\n",
    "            \n",
    "            # Compute adjusted learning rate\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "            \n",
    "            # Update beta\n",
    "            beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_Adagrad(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "print(\"Best model coefficients:\", best_model[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5b4374-61d9-4f4c-b2fa-13c6f2e809e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.00019347]\n",
      " [2.99958109]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad con momentum\n",
    "def plain_Adagrad_momentum(X_train, X_val, y_train, y_val, degree, lmbda, momentum=0.9):\n",
    "    \n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        change = np.zeros_like(beta)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train) + 2 * lmbda * beta\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "                \n",
    "            # Accumulate squared gradients\n",
    "            accumulated_gradients += gradient ** 2\n",
    "            \n",
    "            # Compute adjusted learning rate\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "            \n",
    "            # Update beta using momentum\n",
    "            new_change = adjusted_eta * gradient + momentum * change\n",
    "            beta = beta - new_change\n",
    "            change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_Adagrad_momentum(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "print(\"Best model coefficients:\", best_model[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca68c32-33af-420c-8f5e-08b95dcd9a53",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d854b23c-8fd7-4452-91d3-ca7b961b96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.1 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00030871],\n",
       "       [2.99947435]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plain_RMSprop(X_train, X_val, y_train, y_val, degree, lmbda, decay_rate=0.9, epsilon=1e-8):\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients for RMSprop\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train) + 2 * lmbda * beta\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "                \n",
    "            # Update the accumulated gradients with exponential decay\n",
    "            accumulated_gradients = decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "            \n",
    "            # Compute adjusted learning rate using the accumulated gradients\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + epsilon)\n",
    "            \n",
    "            # Update beta using RMSprop rule\n",
    "            beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "plain_RMSprop(X_train, X_val, y_train, y_val, 1, lmbda)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13473b3d-804e-4012-918e-2d5e83cec298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.01 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00008866],\n",
       "       [2.99963355]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# momentum\n",
    "def plain_RMSprop_momentum(X_train, X_val, y_train, y_val, degree, lmbda, decay_rate=0.9, epsilon=1e-8, momentum=0.9):\n",
    "\n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        change = np.zeros_like(beta)\n",
    "        \n",
    "        # Initialize accumulated squared gradients for RMSprop\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train) + 2 * lmbda * beta\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "                \n",
    "            # Update the accumulated gradients with exponential decay\n",
    "            accumulated_gradients = decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "            \n",
    "            # Compute adjusted learning rate using the accumulated gradients\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + epsilon)\n",
    "            \n",
    "            # Update beta using momentum\n",
    "            new_change = adjusted_eta * gradient + momentum * change\n",
    "            beta = beta - new_change\n",
    "            change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "\n",
    "lmbda = 0.0001\n",
    "plain_RMSprop_momentum(X_train, X_val, y_train, y_val, 1, lmbda)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca324d-d2b8-4b6f-969b-11d5e610dd4d",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4e654c4-1599-4433-90f2-89406fc88f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00011078],\n",
       "       [2.99968926]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam implementation with decay rates for moment estimates\n",
    "def plain_Adam(X_train, X_val, y_train, y_val, degree, lmbda, decay1=0.9, decay2=0.999, epsilon=1e-8):\n",
    "\n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize first moment vector and second moment vector\n",
    "        first_moment = np.zeros_like(beta)  # First moment (mean of gradients)\n",
    "        second_moment = np.zeros_like(beta)  # Second moment (mean of squared gradients)\n",
    "        time_step = 0  # Time step\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            time_step += 1  # Increment the time step\n",
    "            \n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train) + 2 * lmbda * beta\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "                \n",
    "            # Update biased first moment estimate\n",
    "            first_moment = decay1 * first_moment + (1 - decay1) * gradient\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            second_moment = decay2 * second_moment + (1 - decay2) * (gradient ** 2)\n",
    "            \n",
    "            # Correct bias in first moment\n",
    "            first_moment_corrected = first_moment / (1 - decay1 ** time_step)\n",
    "            # Correct bias in second moment\n",
    "            second_moment_corrected = second_moment / (1 - decay2 ** time_step)\n",
    "\n",
    "            # Update beta\n",
    "            beta -= eta * first_moment_corrected / (np.sqrt(second_moment_corrected) + epsilon)\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "plain_Adam(X_train, X_val, y_train, y_val, 1, lmbda)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd1632-d429-4210-b0eb-53352f50690d",
   "metadata": {},
   "source": [
    "## SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b82c7ec-7209-466c-9298-29a3168cbc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.01 with validation MSE = 0.0.\n",
      "Best Model Coefficients: [[2.00010599]\n",
      " [2.99971765]]\n"
     ]
    }
   ],
   "source": [
    "# by hand SGD vero\n",
    "def plain_SGD_hand(X_train, X_val, y_train, y_val, degree, epochs=100, lmbda=0.0001):\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            # Iterate through each training example (SGD)\n",
    "            for x_i, y_i in zip(X_train_poly_shuffled, y_train_shuffled):\n",
    "                x_i = x_i.reshape(-1, 1) \n",
    "\n",
    "                # Calculate the gradient for the single example\n",
    "                gradient = 2.0 * x_i @ (x_i.T @ beta - y_i) + 2 * lmbda * beta\n",
    "\n",
    "                # Define a small tolerance for stopping condition\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "\n",
    "                # Check if the gradient is small enough to stop early\n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "\n",
    "                # Update beta\n",
    "                beta -= eta * gradient\n",
    "\n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "\n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(\"Best Model Coefficients:\", best_model)\n",
    "    return best_model\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_SGD_hand(X_train, X_val, y_train, y_val, degree=1, epochs=1000, lmbda=lmbda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a99a16c6-94bf-4dd7-ac97-6b2125a1a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.00013948, 2.99984557])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit learn SGD\n",
    "def plain_SGD(X_train, X_val, y_train, y_val, degree, lmbda):\n",
    "    best_model_coefficients = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train) \n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Create the SGDRegressor model \n",
    "        sgdreg = SGDRegressor(max_iter=epochs, penalty='l2', alpha=lmbda, eta0=eta, learning_rate='constant', fit_intercept=False)\n",
    "        sgdreg.fit(X_train_poly, y_train.ravel())\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = sgdreg.predict(X_train_poly)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = sgdreg.predict(X_val_poly)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model coefficients based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            # Concatenate intercept and coefficients into a single array\n",
    "            best_model_coefficients = sgdreg.coef_\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model_coefficients, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "plain_SGD(X_train, X_val, y_train, y_val, 1, lmbda)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbeaaf-3d61-4f74-861f-e3fbb084d452",
   "metadata": {},
   "source": [
    "## MGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98346877-faea-494a-95e3-26d64e4a76e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n",
      "Best Model Coefficients: [[2.00012204]\n",
      " [2.99967876]]\n"
     ]
    }
   ],
   "source": [
    "# by hand\n",
    "def plain_SGD_with_mini_batches(X_train, X_val, y_train, y_val, degree, lmbda, batch_size=32, epochs=1000):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "\n",
    "    np.random.seed(32)\n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch) + 2 * lmbda * beta\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Update beta\n",
    "                beta -= eta * gradient\n",
    "\n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "\n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_SGD_with_mini_batches(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "print(\"Best Model Coefficients:\", best_model[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3b775-5d86-45bf-8ed4-2ef69a9c1b67",
   "metadata": {},
   "source": [
    "### momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6a1a314-d85c-4e8b-8431-d0c5f493ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.0001476 ],\n",
       "       [2.99966028]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD batch with momentum\n",
    "def plain_SGD_with_momentum_Ridge(X_train, X_val, y_train, y_val, degree, lmbda, momentum=0.9, batch_size=32, epochs=100):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        change = np.zeros_like(beta)  # Initialize change for momentum\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch) + 2 * lmbda * beta\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                # Update beta using momentum\n",
    "                new_change = eta * gradient + momentum * change\n",
    "                beta = beta - new_change\n",
    "                change = new_change\n",
    "                \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_SGD_with_momentum_Ridge(X_train, X_val, y_train, y_val, degree=1, lmbda = lmbda, momentum=0.9, batch_size=16, epochs=1000)\n",
    "best_model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66540c7-4f9c-4db0-8807-44ea2a437812",
   "metadata": {},
   "source": [
    "### minibatch scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6896e15-d3f6-4098-aae5-7fadd526432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.01 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.00010896, 2.99966085])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit learn minibatch\n",
    "def plain_SGD_with_batches(X_train, X_val, y_train, y_val, degree, lmbda, batch_size=32, epochs=1000):\n",
    "    np.random.seed(32)\n",
    "    best_model_coefficients = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train) \n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize SGDRegressor with the specified learning rate\n",
    "        sgdreg = SGDRegressor(max_iter=1, penalty='l2', alpha=lmbda, eta0=eta, learning_rate='constant', fit_intercept=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            # Iterate over mini-batches\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "                \n",
    "                # Fit the model on the mini-batch\n",
    "                sgdreg.partial_fit(X_batch, y_batch.ravel())\n",
    "\n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = sgdreg.predict(X_train_poly)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = sgdreg.predict(X_val_poly)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model coefficients based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model_coefficients = sgdreg.coef_\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model_coefficients, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = plain_SGD_with_batches(X_train, X_val, y_train, y_val, degree=1, lmbda=lmbda, batch_size=32, epochs=1000)\n",
    "best_model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc7e93-0cd2-4713-9f1f-0071c2854c20",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7b78d8e-14d4-4934-a3fa-8400c0f328ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.1 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.00112959]\n",
      " [2.99870724]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad no memntum\n",
    "def SGD_Adagrad(X_train, X_val, y_train, y_val, degree, lmbda, batch_size=32, epochs=1000):\n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch) + 2 * lmbda * beta\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients += gradient ** 2\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta\n",
    "                beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = SGD_Adagrad(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "print(\"Best model coefficients:\", best_model[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72911a37-0a00-419b-98ad-acb49ad94de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.1 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.00011517]\n",
      " [2.9996565 ]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad momentum  \n",
    "def SGD_Adagrad_momentum(X_train, X_val, y_train, y_val, degree, lmbda, batch_size=32, epochs=1000, momentum=0.9):\n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        # Initialize change for momentum\n",
    "        change = np.zeros_like(beta)  \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch) + 2 * lmbda * beta\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients += gradient ** 2\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta using momentum\n",
    "                new_change = adjusted_eta * gradient + momentum * change\n",
    "                beta = beta - new_change\n",
    "                change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = SGD_Adagrad_momentum(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "print(\"Best model coefficients:\", best_model[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58497be-1e4d-4845-869d-721e9c0f6222",
   "metadata": {},
   "source": [
    "### RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "752f303f-209d-4ca7-bd59-bff7de2f0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.01 with validation MSE = 0.0001.\n",
      "Best model coefficients: [[1.99357078]\n",
      " [2.99716004]]\n"
     ]
    }
   ],
   "source": [
    "# no memntum\n",
    "def SGD_RMSprop(X_train, X_val, y_train, y_val, degree, lmbda, batch_size=32, epochs=1000, decay_rate=0.9):\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch) + 2 * lmbda * beta\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients =  decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta\n",
    "                beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = SGD_RMSprop(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "print(\"Best model coefficients:\", best_model[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fe883fc-b1cc-4507-8a7d-44dfddbc4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.01 with validation MSE = 0.0.\n",
      "Best model coefficients: [[1.99163145]\n",
      " [3.00328533]]\n"
     ]
    }
   ],
   "source": [
    "# momentum\n",
    "def SGD_RMSprop_momentum(X_train, X_val, y_train, y_val, degree, lmbda, batch_size=32, epochs=100, decay_rate=0.9, momentum=0.9):\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        # Initialize change for momentum\n",
    "        change = np.zeros_like(beta)  \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch) + 2 * lmbda * beta\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients =  decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta using momentum\n",
    "                new_change = adjusted_eta * gradient + momentum * change\n",
    "                beta = beta - new_change\n",
    "                change = new_change\n",
    "                \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = SGD_RMSprop_momentum(X_train, X_val, y_train, y_val, 1, lmbda)\n",
    "print(\"Best model coefficients:\", best_model[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707f937-0882-4cb2-b4f7-dc05fb0e9cb0",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ad71ede-ed0b-4308-9ba2-d3f885513ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model having lambda = 0.0001 used eta = 0.2498777664209341 with validation MSE = 0.0.\n",
      "Best Model Coefficients: [[2.0000999 ]\n",
      " [2.99966591]]\n"
     ]
    }
   ],
   "source": [
    "# by hand\n",
    "def SGD_ADAM(X_train, X_val, y_train, y_val, degree, lmbda, batch_size=32, epochs=1000, decay1=0.9, decay2=0.999, epsilon=1e-8):\n",
    "    np.random.seed(32)\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly + 2 * lmbda * np.eye(X_train_poly.shape[1])\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    \n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        # Initialize first moment vector and second moment vector\n",
    "        first_moment = np.zeros_like(beta)  # First moment (mean of gradients)\n",
    "        second_moment = np.zeros_like(beta)  # Second moment (mean of squared gradients)\n",
    "        time_step = 0  # Time step\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                time_step += 1\n",
    "                \n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch) + 2 * lmbda * beta\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Update biased first moment estimate\n",
    "                first_moment = decay1 * first_moment + (1 - decay1) * gradient\n",
    "                \n",
    "                # Update biased second raw moment estimate\n",
    "                second_moment = decay2 * second_moment + (1 - decay2) * (gradient ** 2)\n",
    "                \n",
    "                # Correct bias in first moment\n",
    "                first_moment_corrected = first_moment / (1 - decay1 ** time_step)\n",
    "                # Correct bias in second moment\n",
    "                second_moment_corrected = second_moment / (1 - decay2 ** time_step)\n",
    "    \n",
    "                # Update beta\n",
    "                beta -= eta * first_moment_corrected / (np.sqrt(second_moment_corrected) + epsilon)\n",
    "            \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "\n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    #print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    print(f\"The best model having lambda = {lmbda} used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model, min_mse_val, best_eta\n",
    "\n",
    "lmbda = 0.0001\n",
    "best_model = SGD_ADAM(X_train, X_val, y_train, y_val, degree=1, lmbda=lmbda, batch_size=32, epochs=1000)\n",
    "print(\"Best Model Coefficients:\", best_model[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
