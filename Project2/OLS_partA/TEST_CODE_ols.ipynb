{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f2d9ae-0224-4004-8315-677f85aaf532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f969f342cc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso, SGDRegressor\n",
    "from numpy import asarray\n",
    "\n",
    "# Make data\n",
    "np.random.seed(67)\n",
    "n = 100\n",
    "X = 2 * np.random.rand(n, 1)\n",
    "\n",
    "y = 2 + 3 * X \n",
    "\n",
    "# Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "# Splitting into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=32)\n",
    "\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ce969-49e7-444c-aac2-084f3110c92d",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa67947-af55-498f-9a70-b4284d0a84e6",
   "metadata": {},
   "source": [
    "### plain GD fixed learning rate to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0578e139-e29a-40cd-ba8d-dbe4f565af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n",
      "Best Model Coefficients: [[2.00028635]\n",
      " [2.99973952]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# by hand\n",
    "def plain_GD(X_train, X_val, y_train, y_val, degree):\n",
    "\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "\n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = sorted([0.001, 0.01, 0.1, 1.0 / np.max(EigValues)])\n",
    "    epochs = 1000\n",
    "\n",
    "    mse_train = []\n",
    "    mse_val = []\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "                          \n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train)\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "                            \n",
    "            # Update beta\n",
    "            beta -= eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train.append(mean_squared_error(y_train, y_hat_train))\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val.append(mean_squared_error(y_val, y_hat_val))\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val[-1] < min_mse_val:\n",
    "            min_mse_val = mse_val[-1]\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(\"Best Model Coefficients:\", best_model)\n",
    "    return best_model, mse_train, mse_val, learning_rates\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model, mse_train, mse_val, learning_rates = plain_GD(X_train, X_val, y_train, y_val, 1)\n",
    "\n",
    "# Plot \n",
    "plt.plot(learning_rates, mse_train, label=\"Train\",marker='o')\n",
    "plt.plot(learning_rates, mse_val, label=\"Validation\",marker='o')\n",
    "plt.xlabel(\"Learning rates\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE for training and validation data\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ceac15-7f5d-4e7f-900b-7d64fe90ab4e",
   "metadata": {},
   "source": [
    "### with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220f6e32-4677-44ca-8322-e415fc855b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.1 with validation MSE = 0.0.\n",
      "Best model coefficients: [[1.99999432]\n",
      " [2.9999943 ]]\n"
     ]
    }
   ],
   "source": [
    "def plain_GD_with_momentum_OLS(X_train, X_val, y_train, y_val, degree, momentum=0.9):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        change = np.zeros_like(beta)  # Initialize change for momentum\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train)\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            # Update beta using momentum\n",
    "            new_change = eta * gradient + momentum * change\n",
    "            beta = beta - new_change\n",
    "            change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(\"Best model coefficients:\", best_model)\n",
    "    return best_model\n",
    "\n",
    "# Chiamata della funzione\n",
    "best_model = plain_GD_with_momentum_OLS(X_train, X_val, y_train, y_val, degree=1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcb7f1-5256-4da0-b106-1c29a4b24946",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5e119a-cc48-4d14-b150-24aa8cdadac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.0002765 ]\n",
      " [2.99974304]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad\n",
    "def plain_Adagrad(X_train, X_val, y_train, y_val, degree):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train)\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            accumulated_gradients += gradient ** 2\n",
    "            \n",
    "            # Compute adjusted learning rate\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "            \n",
    "            # Update beta\n",
    "            beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(\"Best model coefficients:\", best_model)\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = plain_Adagrad(X_train, X_val, y_train, y_val, degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5b4374-61d9-4f4c-b2fa-13c6f2e809e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.00008233]\n",
      " [2.99992399]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad con momentum\n",
    "def plain_Adagrad_momentum(X_train, X_val, y_train, y_val, degree, momentum=0.9):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        change = np.zeros_like(beta)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train)\n",
    "            \n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            accumulated_gradients += gradient ** 2\n",
    "            \n",
    "            # Compute adjusted learning rate\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "            \n",
    "            # Update beta using momentum\n",
    "            new_change = adjusted_eta * gradient + momentum * change\n",
    "            beta = beta - new_change\n",
    "            change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(\"Best model coefficients:\", best_model)\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = plain_Adagrad_momentum(X_train, X_val, y_train, y_val, degree=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca68c32-33af-420c-8f5e-08b95dcd9a53",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d854b23c-8fd7-4452-91d3-ca7b961b96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.1 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00019783],\n",
       "       [2.99981604]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plain_RMSprop(X_train, X_val, y_train, y_val, degree, decay_rate=0.9, epsilon=1e-8):\n",
    "\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients for RMSprop\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train)\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            # Update the accumulated gradients with exponential decay\n",
    "            accumulated_gradients = decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "            \n",
    "            # Compute adjusted learning rate using the accumulated gradients\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + epsilon)\n",
    "            \n",
    "            # Update beta using RMSprop rule\n",
    "            beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "plain_RMSprop(X_train, X_val, y_train, y_val, degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13473b3d-804e-4012-918e-2d5e83cec298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.001 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00000842],\n",
       "       [2.99999478]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# momentum\n",
    "def plain_RMSprop_momentum(X_train, X_val, y_train, y_val, degree, decay_rate=0.9, epsilon=1e-8, momentum=0.9):\n",
    "\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        change = np.zeros_like(beta)\n",
    "        \n",
    "        # Initialize accumulated squared gradients for RMSprop\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train)\n",
    "\n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            # Update the accumulated gradients with exponential decay\n",
    "            accumulated_gradients = decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "            \n",
    "            # Compute adjusted learning rate using the accumulated gradients\n",
    "            adjusted_eta = eta / (np.sqrt(accumulated_gradients) + epsilon)\n",
    "            \n",
    "            # Update beta using momentum\n",
    "            new_change = adjusted_eta * gradient + momentum * change\n",
    "            beta = beta - new_change\n",
    "            change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "plain_RMSprop_momentum(X_train, X_val, y_train, y_val, degree=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca324d-d2b8-4b6f-969b-11d5e610dd4d",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e654c4-1599-4433-90f2-89406fc88f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.99999895],\n",
       "       [3.00003084]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam implementation with decay rates for moment estimates\n",
    "def plain_Adam(X_train, X_val, y_train, y_val, degree, decay1=0.9, decay2=0.999, epsilon=1e-8):\n",
    "\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize first moment vector and second moment vector\n",
    "        first_moment = np.zeros_like(beta)  # First moment (mean of gradients)\n",
    "        second_moment = np.zeros_like(beta)  # Second moment (mean of squared gradients)\n",
    "        time_step = 0  # Time step\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            time_step += 1  # Increment the time step\n",
    "            \n",
    "            # Gradient of the cost function\n",
    "            gradient = (2.0 / len(y_train)) * X_train_poly.T @ (X_train_poly @ beta - y_train)\n",
    "            \n",
    "            tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "            if np.all(np.abs(gradient) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            first_moment = decay1 * first_moment + (1 - decay1) * gradient\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            second_moment = decay2 * second_moment + (1 - decay2) * (gradient ** 2)\n",
    "            \n",
    "            # Correct bias in first moment\n",
    "            first_moment_corrected = first_moment / (1 - decay1 ** time_step)\n",
    "            # Correct bias in second moment\n",
    "            second_moment_corrected = second_moment / (1 - decay2 ** time_step)\n",
    "\n",
    "            # Update beta\n",
    "            beta -= eta * first_moment_corrected / (np.sqrt(second_moment_corrected) + epsilon)\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "plain_Adam(X_train, X_val, y_train, y_val, degree=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd1632-d429-4210-b0eb-53352f50690d",
   "metadata": {},
   "source": [
    "## SGD (batch tutto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64cf063-f1cb-4299-aa99-f6c53c3761a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.1 with validation MSE = 0.0.\n",
      "Best Model Coefficients: [[2.0000469 ]\n",
      " [2.99996696]]\n"
     ]
    }
   ],
   "source": [
    "# by hand SGD vero\n",
    "def plain_SGD_hand(X_train, X_val, y_train, y_val, degree, epochs=100):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            # Iterate through each training example (SGD)\n",
    "            for x_i, y_i in zip(X_train_poly_shuffled, y_train_shuffled):\n",
    "                x_i = x_i.reshape(-1, 1) \n",
    "\n",
    "                # Calculate the gradient for the single example\n",
    "                gradient = 2.0 * x_i @ (x_i.T @ beta - y_i)\n",
    "\n",
    "                # Define a small tolerance for stopping condition\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "\n",
    "                # Check if the gradient is small enough to stop early\n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "\n",
    "                # Update beta\n",
    "                beta -= eta * gradient\n",
    "\n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "\n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(\"Best Model Coefficients:\", best_model)\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = plain_SGD_hand(X_train, X_val, y_train, y_val, degree=1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99a16c6-94bf-4dd7-ac97-6b2125a1a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n",
      "Model coefficients: [2. 3.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit learn true SGD \n",
    "def plain_SGD(X_train, X_val, y_train, y_val, degree):\n",
    "    best_model_coefficients = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train) \n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    epochs = 1000\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Create the SGDRegressor model with the specified learning rate\n",
    "        sgdreg = SGDRegressor(max_iter=epochs, penalty=None, eta0=eta, learning_rate='constant', fit_intercept=False)\n",
    "        sgdreg.fit(X_train_poly, y_train.ravel())\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = sgdreg.predict(X_train_poly)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = sgdreg.predict(X_val_poly)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model coefficients based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            # Concatenate intercept and coefficients into a single array\n",
    "            best_model_coefficients = sgdreg.coef_\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    return best_model_coefficients\n",
    "\n",
    "np.random.seed(32)\n",
    "plain_SGD(X_train, X_val, y_train, y_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98346877-faea-494a-95e3-26d64e4a76e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n",
      "Best Model Coefficients: [[2.00008106]\n",
      " [2.99991446]]\n"
     ]
    }
   ],
   "source": [
    "# by hand\n",
    "def plain_SGD_with_mini_batches(X_train, X_val, y_train, y_val, degree, batch_size=32, epochs=100):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch)\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "\n",
    "                # Update beta\n",
    "                beta -= eta * gradient\n",
    "\n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "\n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(\"Best Model Coefficients:\", best_model)\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = plain_SGD_with_mini_batches(X_train, X_val, y_train, y_val, degree=1, batch_size=32, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3b775-5d86-45bf-8ed4-2ef69a9c1b67",
   "metadata": {},
   "source": [
    "### momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a1a314-d85c-4e8b-8431-d0c5f493ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.1 with validation MSE = 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00002131],\n",
       "       [2.99999759]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD batch with momentum\n",
    "def plain_SGD_with_momentum_OLS(X_train, X_val, y_train, y_val, degree, momentum=0.9, batch_size=32, epochs=100):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        \n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        change = np.zeros_like(beta)  # Initialize change for momentum\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch)\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                # Update beta using momentum\n",
    "                new_change = eta * gradient + momentum * change\n",
    "                beta = beta - new_change\n",
    "                change = new_change\n",
    "                \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = plain_SGD_with_momentum_OLS(X_train, X_val, y_train, y_val, degree=1, momentum=0.9, batch_size=32, epochs=1000)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66540c7-4f9c-4db0-8807-44ea2a437812",
   "metadata": {},
   "source": [
    "### minibatch scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6896e15-d3f6-4098-aae5-7fadd526432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n",
      "Model coefficients: [2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Scikit learn minibatch\n",
    "def plain_SGD_with_batches(X_train, X_val, y_train, y_val, degree, batch_size=32, epochs=100):\n",
    "    best_model_coefficients = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train) \n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize SGDRegressor with the specified learning rate\n",
    "        sgdreg = SGDRegressor(max_iter=1, penalty=None, eta0=eta, learning_rate='constant', fit_intercept=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            # Iterate over mini-batches\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "                \n",
    "                # Fit the model on the mini-batch\n",
    "                sgdreg.partial_fit(X_batch, y_batch.ravel())\n",
    "\n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = sgdreg.predict(X_train_poly)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = sgdreg.predict(X_val_poly)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model coefficients based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model_coefficients = sgdreg.coef_\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    print(f\"Model coefficients: {best_model_coefficients}\")\n",
    "    return best_model_coefficients\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model_coefficients = plain_SGD_with_batches(X_train, X_val, y_train, y_val, degree=1, batch_size=32, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc7e93-0cd2-4713-9f1f-0071c2854c20",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b78d8e-14d4-4934-a3fa-8400c0f328ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.1 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.00102288]\n",
      " [2.999045  ]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad no memntum\n",
    "def SGD_Adagrad(X_train, X_val, y_train, y_val, degree, batch_size=32, epochs=100):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch)\n",
    "                \n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients += gradient ** 2\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta\n",
    "                beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = SGD_Adagrad(X_train, X_val, y_train, y_val, degree=1, batch_size=32, epochs=1000)\n",
    "print(\"Best model coefficients:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72911a37-0a00-419b-98ad-acb49ad94de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.2498902548246793 with validation MSE = 0.0.\n",
      "Best model coefficients: [[2.00003201]\n",
      " [2.99997433]]\n"
     ]
    }
   ],
   "source": [
    "# Adagrad with momentum\n",
    "def SGD_Adagrad_momentum(X_train, X_val, y_train, y_val, degree, batch_size=32, epochs=100, momentum=0.9):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        # Initialize change for momentum\n",
    "        change = np.zeros_like(beta)  \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch)\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients += gradient ** 2\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta using momentum\n",
    "                new_change = adjusted_eta * gradient + momentum * change\n",
    "                beta = beta - new_change\n",
    "                change = new_change\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = SGD_Adagrad_momentum(X_train, X_val, y_train, y_val, degree=1, batch_size=32, epochs=1000, momentum=0.9)\n",
    "print(\"Best model coefficients:\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58497be-1e4d-4845-869d-721e9c0f6222",
   "metadata": {},
   "source": [
    "### RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752f303f-209d-4ca7-bd59-bff7de2f0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.01 with validation MSE = 0.0001.\n",
      "Best model coefficients: [[1.99343991]\n",
      " [2.99743597]]\n"
     ]
    }
   ],
   "source": [
    "# no momentum\n",
    "def SGD_RMSprop(X_train, X_val, y_train, y_val, degree, batch_size=32, epochs=100, decay_rate=0.9):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch)\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients =  decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta\n",
    "                beta -= adjusted_eta * gradient\n",
    "        \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = SGD_RMSprop(X_train, X_val, y_train, y_val, degree=1, batch_size=32, epochs=1000, decay_rate=0.9)\n",
    "print(\"Best model coefficients:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe883fc-b1cc-4507-8a7d-44dfddbc4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.001 with validation MSE = 0.0.\n",
      "Best model coefficients: [[1.99994181]\n",
      " [3.00004516]]\n"
     ]
    }
   ],
   "source": [
    "# momentum\n",
    "def SGD_RMSprop_momentum(X_train, X_val, y_train, y_val, degree, batch_size=32, epochs=100, decay_rate=0.9, momentum=0.9):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    \n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        accumulated_gradients = np.zeros_like(beta)\n",
    "\n",
    "        # Initialize change for momentum\n",
    "        change = np.zeros_like(beta)  \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch)\n",
    "                \n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Accumulate squared gradients\n",
    "                accumulated_gradients =  decay_rate * accumulated_gradients + (1 - decay_rate) * (gradient ** 2)\n",
    "                \n",
    "                # Compute adjusted learning rate\n",
    "                adjusted_eta = eta / (np.sqrt(accumulated_gradients) + 1e-8)  # Add small value to prevent division by zero\n",
    "                \n",
    "                # Update beta using momentum\n",
    "                new_change = adjusted_eta * gradient + momentum * change\n",
    "                beta = beta - new_change\n",
    "                change = new_change\n",
    "                \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "    \n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = SGD_RMSprop_momentum(X_train, X_val, y_train, y_val, degree=1, batch_size=32, epochs=1000, decay_rate=0.9, momentum=0.9)\n",
    "print(\"Best model coefficients:\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707f937-0882-4cb2-b4f7-dc05fb0e9cb0",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad71ede-ed0b-4308-9ba2-d3f885513ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model used eta = 0.1 with validation MSE = 0.0.\n",
      "Best Model Coefficients: [[2.00008358]\n",
      " [2.99993017]]\n"
     ]
    }
   ],
   "source": [
    "# by hand\n",
    "def SGD_ADAM(X_train, X_val, y_train, y_val, degree, batch_size=32, epochs=100, decay1=0.9, decay2=0.999, epsilon=1e-8):\n",
    "    best_model = None\n",
    "    best_eta = None\n",
    "    min_mse_val = float('inf')\n",
    "    \n",
    "    # Generate polynomial features for each degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    # Transform the original training and validation sets\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    # Compute the Hessian matrix (based on X_train_poly)\n",
    "    H = (2.0 / len(y_train)) * X_train_poly.T @ X_train_poly\n",
    "    # Get the eigenvalues\n",
    "    EigValues, _ = np.linalg.eig(H)\n",
    "    \n",
    "    # Learning rates to test\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0 / np.max(EigValues)]\n",
    "    \n",
    "    for eta in learning_rates:\n",
    "        # Initialize beta with the correct size based on the degree\n",
    "        beta = np.random.randn(X_train_poly.shape[1], 1)\n",
    "\n",
    "        # Initialize first moment vector and second moment vector\n",
    "        first_moment = np.zeros_like(beta)  # First moment (mean of gradients)\n",
    "        second_moment = np.zeros_like(beta)  # Second moment (mean of squared gradients)\n",
    "        time_step = 0  # Time step\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(len(y_train))\n",
    "            X_train_poly_shuffled = X_train_poly[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(y_train), batch_size):\n",
    "                time_step += 1\n",
    "                \n",
    "                # Create mini-batch\n",
    "                X_batch = X_train_poly_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Calculate the gradient for the mini-batch\n",
    "                gradient = (2.0 / len(y_batch)) * X_batch.T @ (X_batch @ beta - y_batch)\n",
    "\n",
    "                tolerance = np.full_like(gradient, 0.0001)\n",
    "            \n",
    "                if np.all(np.abs(gradient) <= tolerance):\n",
    "                    break\n",
    "                    \n",
    "                # Update biased first moment estimate\n",
    "                first_moment = decay1 * first_moment + (1 - decay1) * gradient\n",
    "                \n",
    "                # Update biased second raw moment estimate\n",
    "                second_moment = decay2 * second_moment + (1 - decay2) * (gradient ** 2)\n",
    "                \n",
    "                # Correct bias in first moment\n",
    "                first_moment_corrected = first_moment / (1 - decay1 ** time_step)\n",
    "                # Correct bias in second moment\n",
    "                second_moment_corrected = second_moment / (1 - decay2 ** time_step)\n",
    "    \n",
    "                # Update beta\n",
    "                beta -= eta * first_moment_corrected / (np.sqrt(second_moment_corrected) + epsilon)\n",
    "            \n",
    "        # TRAIN - Calculate MSE for training data\n",
    "        y_hat_train = np.dot(X_train_poly, beta)\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "    \n",
    "        # VALIDATION - Calculate MSE for validation data\n",
    "        y_hat_val = np.dot(X_val_poly, beta)\n",
    "        mse_val = mean_squared_error(y_val, y_hat_val)\n",
    "\n",
    "        # Save the best model based on validation MSE\n",
    "        if mse_val < min_mse_val:\n",
    "            min_mse_val = mse_val\n",
    "            best_model = beta\n",
    "            best_eta = eta\n",
    "\n",
    "    print(f\"The best model used eta = {best_eta} with validation MSE = {round(min_mse_val, 4)}.\")\n",
    "    return best_model\n",
    "\n",
    "np.random.seed(32)\n",
    "best_model = SGD_ADAM(X_train, X_val, y_train, y_val, degree=1, batch_size=32, epochs=1000, decay1=0.9, decay2=0.999, epsilon=1e-8)\n",
    "print(\"Best Model Coefficients:\", best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
