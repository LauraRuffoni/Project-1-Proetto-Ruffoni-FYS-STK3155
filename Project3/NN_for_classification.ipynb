{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2730ea-b8f0-4563-a92c-fa6cad05b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np  \n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, LinearRegression, Lasso, SGDRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs, load_breast_cancer, make_classification\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a0cb7a9-88d8-4043-9400-cee824f4fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def derivate(func):\n",
    "    return elementwise_grad(func)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "    \n",
    "def binary_cross_entropy(predictions, targets):\n",
    "    predictions = np.clip(predictions, 1e-9, 1 - 1e-9)  # Avoid log(0)\n",
    "    return -np.mean(targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions))\n",
    "\n",
    "def binary_cross_entropy_der(predictions, targets):\n",
    "    predictions = np.clip(predictions, 1e-9, 1 - 1e-9)  # Avoid division by zero\n",
    "    return (predictions - targets) / (predictions * (1 - predictions))\n",
    "\n",
    "def r2_score(predictions, targets):\n",
    "    ss_total = np.sum((targets - np.mean(targets)) ** 2)\n",
    "    ss_residual = np.sum((targets - predictions) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "def create_layers_batch(network_input_size, layer_output_sizes): \n",
    "    layers = []\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(i_size, layer_output_size) * np.sqrt(2 / (i_size + layer_output_size))\n",
    "        b = np.zeros(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = (a @ W) + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def feed_forward_saver_batch(inputs, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = inputs  \n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = a @ W + b  \n",
    "        a = activation_func(z)\n",
    "        zs.append(z)\n",
    "    return layer_inputs, zs, a \n",
    "\n",
    "def backpropagation_batch(input_batch, layers, activation_funcs, targets,activation_ders, cost_der):\n",
    "    batch_size = input_batch.shape[0]\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(input_batch, layers, activation_funcs)\n",
    "    layer_grads = [() for _ in layers]\n",
    "\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "        \n",
    "        if i == len(layers) - 1:\n",
    "            dC_da = cost_der(predict, targets)\n",
    "        else:\n",
    "            (W, b) = layers[i + 1][:2]\n",
    "            dC_da = dC_dz @ W.T \n",
    "        \n",
    "        dC_dz = dC_da * activation_der(z)\n",
    "        dC_dW = layer_input.T @ dC_dz / batch_size\n",
    "        dC_db = np.mean(dC_dz, axis=0)\n",
    "        \n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads\n",
    "\n",
    "def train_nn_classification(inputs, targets, val_inputs, val_targets,test_inputs, test_targets, layers, activation_funcs, activation_ders, learning_rate=0.001, epochs=100, batch_size=32):\n",
    "    n_samples = len(inputs)\n",
    "    training_metrics = {'loss': [], 'accuracy': []}\n",
    "    validation_metrics = {'loss': [], 'accuracy': []}\n",
    "    test_metrics = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        inputs_shuffled = inputs[indices]\n",
    "        targets_shuffled = targets[indices]\n",
    "        # Mini-batch training\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_inputs = inputs_shuffled[i:i + batch_size]\n",
    "            batch_targets = targets_shuffled[i:i + batch_size]\n",
    "            # Compute gradients\n",
    "            layer_grads = backpropagation_batch(batch_inputs, layers, activation_funcs, batch_targets, activation_ders, binary_cross_entropy_der)\n",
    "            # Update weights and biases\n",
    "            for j, ((W, b), (dW, db)) in enumerate(zip(layers, layer_grads)):\n",
    "                layers[j] = (W - learning_rate * dW, b - learning_rate * db)\n",
    "                \n",
    "        if epoch % 10 == 0:\n",
    "            predictions = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "            acc = accuracy_score(targets, (predictions > 0.5).astype(int))\n",
    "            print(f'Epoch {epoch}, Training Accuracy: {acc:.4f}')\n",
    "        \n",
    "        # Calculate metrics on training set\n",
    "        predictions_train = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "        loss_train = binary_cross_entropy(predictions_train, targets)\n",
    "        accuracy_train = accuracy_score(targets, (predictions_train > 0.5).astype(int))\n",
    "        training_metrics['loss'].append(loss_train)\n",
    "        training_metrics['accuracy'].append(accuracy_train)\n",
    "\n",
    "        # Calculate metrics on validation set\n",
    "        predictions_val = feed_forward_batch(val_inputs, layers, activation_funcs)\n",
    "        loss_val = binary_cross_entropy(predictions_val, val_targets)\n",
    "        accuracy_val = accuracy_score(val_targets, (predictions_val > 0.5).astype(int))\n",
    "        validation_metrics['loss'].append(loss_val)\n",
    "        validation_metrics['accuracy'].append(accuracy_val)\n",
    "\n",
    "        # Calculate metrics on test set\n",
    "        predictions_test = feed_forward_batch(test_inputs, layers, activation_funcs)\n",
    "        loss_test = binary_cross_entropy(predictions_test, test_targets)\n",
    "        accuracy_test = accuracy_score(test_targets, (predictions_test > 0.5).astype(int))\n",
    "        test_metrics['loss'].append(loss_test)\n",
    "        test_metrics['accuracy'].append(accuracy_test)\n",
    "\n",
    "    return layers, training_metrics, validation_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3e78e-15bd-42e6-88e7-95770b795d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(32)\n",
    "# Load and prepare data\n",
    "data = load_breast_cancer()\n",
    "X = data.data \n",
    "y = data.target\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoding for compatibility \n",
    "y_train_onehot = np.eye(2)[y_train]\n",
    "y_val_onehot = np.eye(2)[y_val]\n",
    "\n",
    "# Network configuration\n",
    "input_size = X_train.shape[1]\n",
    "layer_output_sizes = [50, 2]  # Output layer has 2 neurons for binary classification \n",
    "activation_funcs = [relu, sigmoid]\n",
    "activation_ders = [relu_derivative, sigmoid_der]\n",
    "\n",
    "# Create layers\n",
    "layers = create_layers_batch(input_size, layer_output_sizes)\n",
    "\n",
    "# Get initial predictions and accuracy\n",
    "initial_predictions = feed_forward_batch(X_val, layers, activation_funcs)\n",
    "initial_pred_classes = np.argmax(initial_predictions, axis=1) \n",
    "initial_acc = accuracy_score(y_val, initial_pred_classes)\n",
    "print(f'Initial accuracy before training: {initial_acc:.4f}')\n",
    "\n",
    "# Confusion matrix before training\n",
    "initial_conf_matrix = confusion_matrix(y_val, initial_pred_classes)\n",
    "print(\"\\nConfusion Matrix before training on validation:\")\n",
    "print(initial_conf_matrix)\n",
    "\n",
    "# Train the network\n",
    "layers, training_metrics, validation_metrics = train_nn_classification(X_train, y_train_onehot, X_val, y_val_onehot, layers, activation_funcs, activation_ders, learning_rate=0.01, epochs=100, batch_size=32)\n",
    "\n",
    "# Get final predictions and accuracy\n",
    "final_predictions = feed_forward_batch(X_val, layers, activation_funcs)\n",
    "final_pred_classes = np.argmax(final_predictions, axis=1) \n",
    "final_acc = accuracy_score(y_val, final_pred_classes)\n",
    "print(f'\\nFinal accuracy after training on validation: {final_acc:.4f}')\n",
    "\n",
    "# Confusion matrix after training\n",
    "final_conf_matrix = confusion_matrix(y_val, final_pred_classes)\n",
    "print(\"\\nConfusion Matrix after training on validation:\")\n",
    "print(final_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bff2256-34f0-4226-893a-df32cbb2f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize the images\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Loading the dataset using ImageFolder\n",
    "dataset_path = '/home/laura/Desktop/OsloUni/ML/Project3/archive/'\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "# The ImageFolder class assumes the dataset is organized into subfolders representing class labels.\n",
    "# The `transform` argument ensures that all images undergo the specified transformations.\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.7 * len(dataset))  # 70% of the dataset for training\n",
    "val_size = int(0.15 * len(dataset))   # 15% of the dataset for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Creating DataLoaders for each set\n",
    "trainloader = DataLoader(train_set, batch_size=32, shuffle=True)  # Shuffle the training data for randomness\n",
    "valloader = DataLoader(val_set, batch_size=32, shuffle=False)     # No shuffling for validation\n",
    "testloader = DataLoader(test_set, batch_size=32, shuffle=False)   # No shuffling for testing\n",
    "\n",
    "# converts the DataLoader objects to NumPy arrays, which are more suitable for scikit-learn models such as logistic regression\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for images, labels in dataloader:\n",
    "        images = images.view(images.size(0), -1).numpy()  # Flatten images\n",
    "        # This flattens each image into a 1D vector. \n",
    "        # The images are initially in the shape (batch_size, 3, 128, 128) (batch size, channels, height, width). \n",
    "        # After flattening, the shape becomes (batch_size, 3 * 128 * 128)\n",
    "        data_list.append(images)\n",
    "        label_list.append(labels.numpy())\n",
    "    return np.vstack(data_list), np.hstack(label_list)\n",
    "\n",
    "X_train, y_train = dataloader_to_numpy(trainloader)\n",
    "X_val, y_val = dataloader_to_numpy(valloader)\n",
    "X_test, y_test = dataloader_to_numpy(testloader)\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "324c8e4e-d848-408e-92cc-31c1f6fa8cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy before training: 0.3846\n",
      "\n",
      "Confusion Matrix before training on validation:\n",
      "[[ 44 234]\n",
      " [ 70 146]]\n",
      "Epoch 0, Training Accuracy: 0.7256\n",
      "Epoch 10, Training Accuracy: 0.8101\n",
      "Epoch 20, Training Accuracy: 0.7720\n",
      "\n",
      "Final accuracy after training on validation: 0.7895\n",
      "\n",
      "Confusion Matrix after training on validation:\n",
      "[[218  60]\n",
      " [ 44 172]]\n",
      "\n",
      "Final accuracy after training on test: 0.8145\n",
      "\n",
      "Confusion Matrix after training on test:\n",
      "[[226  53]\n",
      " [ 39 178]]\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train and y_test to one-hot encoding for compatibility \n",
    "y_train_onehot = np.eye(2)[y_train]\n",
    "y_val_onehot = np.eye(2)[y_val]\n",
    "y_test_onehot = np.eye(2)[y_test]\n",
    "\n",
    "# Network configuration\n",
    "input_size = X_train.shape[1]\n",
    "layer_output_sizes = [16, 32, 64, 2]  # Output layer has 2 neurons for binary classification \n",
    "activation_funcs = [relu, relu, relu, sigmoid]\n",
    "activation_ders = [relu_derivative, relu_derivative, relu_derivative, sigmoid_der]\n",
    "\n",
    "# Create layers\n",
    "layers = create_layers_batch(input_size, layer_output_sizes)\n",
    "\n",
    "# Get initial predictions and accuracy\n",
    "initial_predictions = feed_forward_batch(X_val, layers, activation_funcs)\n",
    "initial_pred_classes = np.argmax(initial_predictions, axis=1) \n",
    "initial_acc = accuracy_score(y_val, initial_pred_classes)\n",
    "print(f'Initial accuracy before training: {initial_acc:.4f}')\n",
    "\n",
    "# Confusion matrix before training\n",
    "initial_conf_matrix = confusion_matrix(y_val, initial_pred_classes)\n",
    "print(\"\\nConfusion Matrix before training on validation:\")\n",
    "print(initial_conf_matrix)\n",
    "\n",
    "# Train the network\n",
    "layers, training_metrics, validation_metrics, test_metrics = train_nn_classification(X_train, y_train_onehot, X_val, y_val_onehot, X_test, y_test_onehot, layers, activation_funcs, activation_ders, learning_rate=0.001, epochs=30, batch_size=32)\n",
    "\n",
    "# Get final predictions and accuracy\n",
    "final_predictions = feed_forward_batch(X_val, layers, activation_funcs)\n",
    "final_pred_classes = np.argmax(final_predictions, axis=1) \n",
    "final_acc = accuracy_score(y_val, final_pred_classes)\n",
    "print(f'\\nFinal accuracy after training on validation: {final_acc:.4f}')\n",
    "\n",
    "# Confusion matrix after training\n",
    "final_conf_matrix = confusion_matrix(y_val, final_pred_classes)\n",
    "print(\"\\nConfusion Matrix after training on validation:\")\n",
    "print(final_conf_matrix)\n",
    "\n",
    "# Test\n",
    "# Get final predictions and accuracy\n",
    "final_predictions = feed_forward_batch(X_test, layers, activation_funcs)\n",
    "final_pred_classes = np.argmax(final_predictions, axis=1) \n",
    "final_acc = accuracy_score(y_test, final_pred_classes)\n",
    "print(f'\\nFinal accuracy after training on test: {final_acc:.4f}')\n",
    "\n",
    "# Confusion matrix after training\n",
    "final_conf_matrix = confusion_matrix(y_test, final_pred_classes)\n",
    "print(\"\\nConfusion Matrix after training on test:\")\n",
    "print(final_conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
